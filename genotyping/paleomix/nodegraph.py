#!/usr/bin/python3
#
# Copyright (c) 2012 Mikkel Schubert <MikkelSch@gmail.com>
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
import collections
import errno
import logging
import os
from typing import (
    Callable,
    Collection,
    Dict,
    Iterable,
    Iterator,
    List,
    Optional,
    Set,
    Tuple,
)

from paleomix.common.fileutils import missing_executables, missing_files
from paleomix.common.utilities import safe_coerce_to_frozenset
from paleomix.common.versions import Requirement, RequirementError
from paleomix.node import Node


class FileStatusCache:
    """Cache used to avoid repeatedly checking the state (existance / mtime) of
    files required / generated by nodes. A new cache is generated for every
    operation (e.g. refreshing all states / manually setting the state of a
    node) to avoid relying on the filesystem staying consistant for long
    periods of time.
    """

    def __init__(self):
        self._stat_cache: Dict[str, Optional[float]] = {}

    def files_exist(self, fpaths: Iterable[str]) -> bool:
        """Returns true if all paths listed in fpaths exist."""
        return all((self._get_state(fpath) is not None) for fpath in fpaths)

    def missing_files(self, fpaths: Iterable[str]) -> List[str]:
        """Returns a list of paths in fpaths that do not exist."""
        return [fpath for fpath in fpaths if (self._get_state(fpath) is None)]

    def are_files_outdated(
        self,
        input_files: Iterable[str],
        output_files: Iterable[str],
    ) -> bool:
        """Returns true if any 'input' files have a time-stamp that post-date
        any time-stamp for the 'output' files, indicating that one or more of
        the 'input' files have changed since the creation of the 'output'.

        The function also returns true if any files are missing, as this would
        indicate either the 'output' files or both the 'input' and 'output'
        files would need to be rebuilt.
        """
        input_timestamps: List[float] = []
        if not self._get_states(input_files, input_timestamps):
            return True

        output_timestamps: List[float] = []
        if not self._get_states(output_files, output_timestamps):
            return True

        return max(input_timestamps) > min(output_timestamps)

    def _get_states(self, filenames: Iterable[str], dst: List[float]) -> bool:
        """Collects the mtimes for a set of filenames, returning true if all
        could be collected, and aborting early and returning false otherwise.
        """
        for filename in filenames:
            timestamp = self._get_state(filename)
            if timestamp is None:
                return False

            dst.append(timestamp)

        return True

    def _get_state(self, fpath: str) -> Optional[float]:
        """Returns the mtime of a path, or None if the path does not exist."""
        if fpath not in self._stat_cache:
            try:
                mtime = os.path.getmtime(fpath)
            except OSError as error:
                if error.errno != errno.ENOENT:
                    raise
                mtime = None
            self._stat_cache[fpath] = mtime
        return self._stat_cache[fpath]


class NodeGraphError(RuntimeError):
    pass


class NodeGraph:
    NUMBER_OF_STATES = 6
    DONE, RUNNING, RUNABLE, QUEUED, OUTDATED, ERROR = range(NUMBER_OF_STATES)

    def __init__(
        self,
        nodes: Iterable[Node],
        implicit_dependencies: bool = False,
        cache_factory: Callable[[], FileStatusCache] = FileStatusCache,
    ):
        self._implicit_dependencies = implicit_dependencies
        self._cache_factory = cache_factory
        self._states: Dict[Node, int] = {}
        self._state_counts = [0] * self.NUMBER_OF_STATES

        nodes = safe_coerce_to_frozenset(nodes)

        self._intersections_cache: Dict[Node, Dict[Node, int]] = {}
        self._dependencies = self._collect_dependencies(nodes)
        self._reverse_dependencies = self._collect_reverse_dependencies(
            self._dependencies
        )

        self._logger = logging.getLogger(__name__)
        self._logger.info("Checking file dependencies")
        if not self._check_file_dependencies(self._reverse_dependencies):
            raise NodeGraphError("Aborting due to input/output file error")

        self._logger.info("Checking for auxiliary files")
        if not self._check_auxiliary_files(self._reverse_dependencies):
            raise NodeGraphError(
                "Please refer to the PALEOMIX installation instructions at "
                "https://paleomix.readthedocs.io/en/stable/"
            )

        self.requirements = self._collect_requirements(self._reverse_dependencies)

        self._logger.info("Determining state of pipeline tasks")
        self._refresh_states()

    def get_node_state(self, node: Node) -> int:
        return self._states[node]

    def set_node_state(self, node: Node, state: int) -> Iterator[Tuple[Node, int, int]]:
        old_state = self._states[node]
        if state == old_state:
            return

        self._states[node] = state
        self._state_counts[old_state] -= 1
        self._state_counts[state] += 1

        yield node, old_state, state

        intersections = self._calculate_intersections(node)

        # Not all nodes may need to be updated, but we still need to
        # traverse the "graph" (using the intersection counts) in order
        # to ensure that all nodes that need to be updated are updated.
        requires_update = dict.fromkeys(intersections, False)
        for dependency in self._reverse_dependencies[node]:
            requires_update[dependency] = True

        cache = self._cache_factory()
        while any(requires_update.values()):
            for (node, count) in tuple(intersections.items()):
                if not count:
                    has_changed = False
                    if requires_update[node]:
                        old_state = self._states.pop(node)
                        new_state = self._update_node_state(node, cache)
                        has_changed |= new_state != old_state

                        self._state_counts[old_state] -= 1
                        self._state_counts[new_state] += 1
                        yield node, old_state, new_state

                    for dependency in self._reverse_dependencies[node]:
                        intersections[dependency] -= 1
                        requires_update[dependency] |= has_changed

                    intersections.pop(node)
                    requires_update.pop(node)

    def iterflat(self) -> Iterator[Node]:
        return iter(self._reverse_dependencies)

    def get_state_counts(self):
        return list(self._state_counts)

    def _refresh_states(self):
        states: Dict[Node, int] = {}
        cache = self._cache_factory()
        for (node, state) in self._states.items():
            if state in (self.ERROR, self.RUNNING):
                states[node] = state
        self._states = states

        for node in self._reverse_dependencies:
            self._update_node_state(node, cache)

        state_counts = [0] * self.NUMBER_OF_STATES
        for state in states.values():
            state_counts[state] += 1
        self._state_counts = state_counts

    def _calculate_intersections(self, for_node: Node) -> Dict[Node, int]:
        def count_nodes(node: Node, counts: Dict[Node, int]) -> Dict[Node, int]:
            for node in self._reverse_dependencies[node]:
                if node in counts:
                    counts[node] += 1
                else:
                    counts[node] = 1
                    count_nodes(node, counts)
            return counts

        if for_node not in self._intersections_cache:
            counts = count_nodes(for_node, {})
            for dependency in self._reverse_dependencies[for_node]:
                counts[dependency] -= 1
            self._intersections_cache[for_node] = counts

        return dict(self._intersections_cache[for_node])

    def _update_node_state(self, node: Node, cache: FileStatusCache) -> int:
        if node in self._states:
            return self._states[node]

        # Update sub-nodes before checking for fixed states
        dependency_states = set((NodeGraph.DONE,))
        for dependency in self._dependencies[node]:
            dependency_states.add(self._update_node_state(dependency, cache))

        state = max(dependency_states)
        if state == NodeGraph.DONE:
            if not self.is_done(node, cache):
                state = NodeGraph.RUNABLE
            elif not cache.files_exist(node.input_files):
                # Somehow the input files have gone missing, despite the
                # dependant nodes being done; this implies this node is
                # outdated, since the input-files should be re-generated, but
                # obviously it is not possible to run it at this point.
                missing = cache.missing_files(node.input_files)
                self._logger.error(
                    "Input file(s) missing for node; may have been moved while the "
                    "pipeline was running. Cannot proceed:\n"
                    "    Node = %s\n    Files = %s\n",
                    node,
                    "\n            ".join(missing),
                )
                state = NodeGraph.ERROR
            elif self.is_outdated(node, cache):
                state = NodeGraph.RUNABLE
        elif state in (NodeGraph.RUNNING, NodeGraph.RUNABLE, NodeGraph.QUEUED):
            if self.is_done(node, cache):
                state = NodeGraph.OUTDATED
            else:
                state = NodeGraph.QUEUED
        self._states[node] = state

        return state

    @classmethod
    def is_done(cls, node: Node, cache: FileStatusCache) -> bool:
        """Returns true if the node itself is done; this only implies that the
        output files generated by this node exists. The files themselves may
        be outdated.
        """
        return cache.files_exist(node.output_files)

    @classmethod
    def is_outdated(cls, node: Node, cache: FileStatusCache) -> int:
        """Returns true if the not is not done or if one or more of the input
        files appear to have been changed since the creation of the output
        files (based on the timestamps). A node that lacks either input or
        output files is never considered outdated.
        """
        if not (node.input_files and node.output_files):
            return False

        return cache.are_files_outdated(node.input_files, node.output_files)

    @classmethod
    def _collect_requirements(cls, nodes: Iterable[Node]) -> Tuple[Requirement, ...]:
        executables: Set[str] = set()
        requirements: Set[Requirement] = set()
        for node in nodes:
            executables.update(node.executables)
            requirements.update(node.requirements)

        # Executables used by requirement checks
        requirement_execs: Set[str] = set()
        for requirement in requirements:
            executable = requirement.executable
            if executable is not None:
                requirement_execs.add(executable)

        # Create dummy Requirements for any executables used in commands but not in reqs
        for executable in executables - requirement_execs:
            requirement = Requirement(executable)
            # Handle the presence of "%(PYTHON)s"
            if requirement.executable not in requirement_execs:
                requirements.add(requirement)

        return tuple(sorted(requirements, key=lambda req: req.name))

    @classmethod
    def check_version_requirements(
        cls,
        requirements: Iterable[Requirement],
        force: bool = False,
    ) -> bool:
        executables = set(requirement.executable for requirement in requirements)
        missing_execs = missing_executables(executables - set([None]))

        any_errors = False
        log = logging.getLogger(__name__)
        for requirement in requirements:
            name = requirement.name
            if requirement.executable in missing_execs:
                if requirement.name != requirement.executable:
                    name = f"Executable for {name} ({requirement.executable!r})"

                log.error(" [☓] %s not found", name)
                any_errors = True
                continue

            try:
                version = requirement.version(force)
                if version:
                    name = "%s v%s" % (name, version)

                if requirement.check():
                    log.info("  [✓] %s ", name)
                else:
                    any_errors = True
                    log.error(
                        " [☓] %s found but %s is required", name, requirement.specifiers
                    )
            except OSError as error:
                any_errors = True
                log.error(" [☓] %s: %s", name, error)
            except RequirementError as error:
                any_errors = True
                log.error(" [☓] %s: %s", name, "\n     ".join(str(error).split("\n")))

        if any_errors:
            log.error(
                "Please refer to the PALEOMIX installation instructions at\n"
                "  https://paleomix.readthedocs.io/en/stable/"
            )

        return not any_errors

    def _check_file_dependencies(self, nodes: Iterable[Node]) -> bool:
        def _abs_path(filename: str, cache: Dict[str, str] = {}) -> str:
            filepath = cache.get(filename)
            if filepath is None:
                filepath = cache[filename] = os.path.abspath(filename)

            return filepath

        input_files: Dict[str, Set[Node]] = collections.defaultdict(set)
        output_files: Dict[str, Set[Node]] = collections.defaultdict(set)
        for node in nodes:
            for filename in node.input_files:
                input_files[_abs_path(filename)].add(node)

            for filename in node.output_files:
                output_files[_abs_path(filename)].add(node)

        output_bugs = self._check_output_files(output_files)
        input_bugs, input_errors = self._check_input_files(input_files, output_files)

        if output_bugs or input_bugs:
            self._logger.error(
                "This is probably a bug in PALEOMIX; please report at "
                "https://github.com/MikkelSchubert/paleomix/issues/new"
            )

        return not (output_bugs or input_bugs or input_errors)

    def _check_output_files(self, output_files: Dict[str, Set[Node]]) -> bool:
        """Checks dict of output files to nodes for cases where
        multiple nodes create the same output file.

        The directory component of paths are realized in order to
        detect cases where nodes create the same file, but via
        different paths (e.g. due to relative/absolute paths, or
        due to use of symbolic links). Since output files are
        replaced, not modified in place, it is not nessesary to
        compare files themselves."""
        any_bugs = False
        for filename, nodes in output_files.items():
            if len(nodes) > 1:
                any_bugs = True
                self._logger.error("Multiple tasks write to file %r", filename)
                self._logger.debug("depended on by")
                for line in _summarize_nodes(nodes):
                    self._logger.debug("  %s", line)

        return any_bugs

    def _check_input_files(
        self,
        input_files: Dict[str, Set[Node]],
        output_files: Dict[str, Set[Node]],
    ) -> Tuple[bool, bool]:
        any_bugs = False
        any_user_errors = False
        for (filename, nodes) in sorted(input_files.items(), key=lambda v: v[0]):
            if filename in output_files:
                producers = tuple(output_files[filename])
                # Output files being clobbered is handled in `_check_output_files`
                if len(producers) > 1:
                    continue

                producer = producers[0]
                bad_nodes: Set[Node] = set()
                for consumer in nodes:
                    if self._implicit_dependencies:
                        self._dependencies[consumer].add(producer)
                        self._reverse_dependencies[producer].add(consumer)
                    elif not self._dependency_in(producer, consumer.dependencies):
                        bad_nodes.add(consumer)

                if bad_nodes:
                    any_bugs = True
                    self._logger.error(
                        "Tasks depends on file, but not on the task creating it: %r",
                        filename,
                    )

                    self._logger.debug("  produced when %s", producer)
                    self._logger.debug("  required when")
                    for line in _summarize_nodes(bad_nodes):
                        self._logger.debug("    %s", line)
            elif not os.path.exists(filename):
                any_user_errors = True
                self._logger.error("Required input file does not exist: %r", filename)
                for line in _summarize_nodes(nodes):
                    self._logger.debug("  required when %s", line)

        return any_bugs, any_user_errors

    def _check_auxiliary_files(self, nodes: Iterable[Node]) -> bool:
        auxiliary_files: Set[str] = set()
        for node in nodes:
            auxiliary_files.update(node.auxiliary_files)

        missing_aux_files = missing_files(auxiliary_files)
        if missing_aux_files:
            self._logger.error("Required files not found:")
            for name in sorted(missing_aux_files):
                self._logger.error(" - %s", name)

        return not missing_aux_files

    @classmethod
    def _collect_dependencies(cls, nodes: Iterable[Node]):
        dependencies: Dict[Node, Set[Node]] = {}
        pending = list(nodes)

        while pending:
            node = pending.pop()
            if node not in dependencies:
                dependencies[node] = set(node.dependencies)
                pending.extend(node.dependencies)

        return dependencies

    @classmethod
    def _collect_reverse_dependencies(cls, nodes: Iterable[Node]):
        rev_dependencies: Dict[Node, Set[Node]] = {node: set() for node in nodes}

        for dependant_node in rev_dependencies:
            for node in dependant_node.dependencies:
                rev_dependencies[node].add(dependant_node)

        return rev_dependencies

    @classmethod
    def _dependency_in(cls, dependency: Node, nodes: Collection[Node]) -> bool:
        if dependency in nodes:
            return True

        # Dependencies are mostly one or two levels down, so do a breadth-first search
        checked: Set[Node] = set()
        while nodes:
            pending: Set[Node] = set()
            for node in nodes:
                pending.update(node.dependencies)

            if dependency in pending:
                return True

            checked.update(nodes)
            nodes = pending - checked

        return False


def _summarize_nodes(nodes: Iterable[Node]) -> List[str]:
    lines = list(sorted(set(map(str, nodes))))
    if len(lines) > 4:
        lines = lines[:5] + ["and %i more nodes" % len(lines)]
    return lines
